{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOxH9LqdXOpMUwhXEfH3iDI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zqstPH1tBDF5"},"outputs":[],"source":[]},{"cell_type":"code","source":["!kaggle datasets download -d snap/amazon-fine-food-reviews"],"metadata":{"id":"KTYd-yg-QDQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"],"metadata":{"id":"p0ZKP8ehQKow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"metadata":{"id":"ax2Noh3tQyQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/amazon-fine-food-reviews.zip"],"metadata":{"id":"gHxLyiBVQ57E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sqlite3\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"_kfgqJWcR58T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["con = sqlite3.connect('/content/database.sqlite')"],"metadata":{"id":"v8idlg3dR-Cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B53kLLYMT-yA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3 LIMIT 5000\"\"\",con)"],"metadata":{"id":"GvEkEqJzSEhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"r4-qEBAISQ56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"hicHiUvaSx7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filter = lambda x: 0 if x<3 else 1\n","df[\"Score\"] = df[\"Score\"].map(filter)\n","df.head(3)"],"metadata":{"id":"7v3vbMFkS2dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Score'].value_counts()"],"metadata":{"id":"ZXlzV6-nTgdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uc = pd.read_sql_query(\"\"\"\n","select *,count(*) from Reviews where Score!=3 group by UserId having count(*)>1\n","\n","\"\"\",con)\n","uc.head()"],"metadata":{"id":"-kbLziA0UXYz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uc[uc['count(*)']==uc['count(*)'].max()]"],"metadata":{"id":"aH3yUKZrVCKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df['UserId'] == \"A3OXHLG6DIBRW8\"].shape"],"metadata":{"id":"dIRsy59oVl0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df['UserId'].isin((df[df[\"Time\"].duplicated()][\"UserId\"].value_counts() > 1).index)]"],"metadata":{"id":"kij-A2wOWJKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df[\"UserId\"] == \"AR5J8UI46CURR\"][\"Text\"]"],"metadata":{"id":"S1ABrdsCXadt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf = pd.read_sql_query(\"\"\"\n","select * from Reviews where Score != 3\n","\"\"\",con)"],"metadata":{"id":"TGHj5huKX9XI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qq = cdf.groupby(['Time','UserId',\"ProductId\"])"],"metadata":{"id":"lkVR2ABhaKfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qq.count()[qq.count()[\"ProfileName\"]==11]"],"metadata":{"id":"nFLpa1ZzaZG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cdf[cdf[\"UserId\"] == \"A29JUMRL1US6YP\"].sort_values([\"Time\",\"ProductId\"])"],"metadata":{"id":"H652CncZahje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","documents = [\"The quick brown fox jumps over the lazy dog\",\n","             \"The dog is slow and the fox is very quick\"]\n","vectorizer = CountVectorizer()\n","mat = vectorizer.fit_transform(documents)\n","# matrix = vectorizer.transform(documents)\n"],"metadata":{"id":"FBHgXbnFbgik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mat,type(mat))"],"metadata":{"id":"oQnWY2ad2rkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mat.shape"],"metadata":{"id":"isDGNIQb2tl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","complete = [i.lower().split(\" \") for i in documents]\n","complete"],"metadata":{"id":"DjTVlQWN3sgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["store = {}\n","for i in documents:\n","  for j in i.lower().split(' '):\n","    store.setdefault(j,0)\n","    store[j]+=1"],"metadata":{"id":"SW37HZSG3LuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = list(store.keys())\n","words"],"metadata":{"id":"3iJgWiz23nzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indeces = {words[i]:i for i in range(len(words))}\n","indeces"],"metadata":{"id":"DoAI3NTM5XzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","a = np.zeros((len(documents),len(words)),dtype=int)\n","ip = 0\n","for i in documents:\n","  for j in i.lower().split(\" \"):\n","    a[ip][indeces[j]] +=1\n","  ip+=1\n"],"metadata":{"id":"97gRz8ym5qpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"BcJznZtS7wxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mat,mat.get_shape())"],"metadata":{"id":"8okMnSn48sad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyCountVectorizer:\n","    def __init__(self):\n","        self.vocabulary_ = {}  # To store word counts and their indices\n","\n","    def fit_transform(self, documents):\n","        # Tokenize documents (split into words)\n","        tokenized_docs = []\n","        for doc in documents:\n","            tokenized_docs.append(doc.lower().split())  # Lowercase and split\n","\n","        # Build vocabulary (unique words and their counts)\n","        vocabulary = {}\n","        for doc in tokenized_docs:\n","            for word in doc:\n","                if word not in vocabulary:\n","                    vocabulary[word] = 0\n","                vocabulary[word] += 1\n","\n","        # Assign unique indices to words in the vocabulary\n","        self.vocabulary_ = {word: i for i, word in enumerate(vocabulary)}\n","\n","        # Create a sparse matrix to store term frequencies (replace with actual sparse matrix implementation)\n","        term_frequency_matrix = []\n","        for doc in tokenized_docs:\n","            frequency_vector = [0] * len(self.vocabulary_)  # Initialize with zeros\n","            for word in doc:\n","                if word in self.vocabulary_:\n","                    frequency_vector[self.vocabulary_[word]] += 1  # Increment count for each word\n","            term_frequency_matrix.append(frequency_vector)\n","\n","        return term_frequency_matrix\n","\n","# Example usage\n","documents = [\"The quick brown fox jumps over the lazy dog\",\n","              \"The dog is slow and the fox is very quick\"]\n","\n","vectorizer = MyCountVectorizer()\n","term_frequency_matrix = vectorizer.fit_transform(documents)\n","\n","# Print the term frequency matrix (replace with actual sparse matrix access methods)\n","for doc in term_frequency_matrix:\n","    print(doc)\n"],"metadata":{"id":"-kpc5OkC9EaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def give_counts():\n","  count = []\n","  ind= {}\n","  words = []\n","  id=0\n","  for i in documents:\n","    ss = {}\n","    for j in i.lower().split(\" \"):\n","      ind.setdefault(j,-1)\n","      if ind[j] == -1:\n","        ind[j] = id\n","        words.append(j)\n","        id+=1\n","      ss.setdefault(ind[j],0)\n","      ss[ind[j]]+=1\n","\n","    count.append(ss)\n","\n","\n","  return words,count,ind\n","\n"],"metadata":{"id":"b7kM9WSFF0eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w,c,inx = give_counts()\n","print(w,inx)\n","\n","for i in range(len(c)):\n","  for k,v in c[i].items():\n","    print(f\"({i},{k}) {v}\")\n","  print(\"--\"*10)\n","\n","\n"],"metadata":{"id":"C3sgSzu9Gy7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Multy Thredding"],"metadata":{"id":"J9WEKEhgHXc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from threading import *\n","import time\n","\n","\n","l = Lock()\n","def function(name):\n","  l.acquire()\n","  for i in range(10):\n","    print(\"Good Morning \",end=\"\")\n","    time.sleep(2)\n","    print(name)\n","  l.release()\n","\n","th1 = Thread(target=function,args=(\"Youraj\",))\n","th2 = Thread(target=function,args=(\"Dhonei\",))\n","\n","th1.start()\n","th2.start()\n","\n","# for i in range(10):\n","#   print(\"Main Thred {}\".format(i))"],"metadata":{"id":"gXJ0fplIia-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","from tqdm import tqdm\n","\n","\n","def download_file(url, filename):\n","  \"\"\"Downloads a file from the specified URL and saves it with the given filename,\n","  displaying a progress bar during the download.\n","\n","  Args:\n","      url: The URL of the file to download.\n","      filename: The name of the file to save the downloaded content as.\n","  \"\"\"\n","\n","  # Check if the 'requests' and 'tqdm' libraries are installed\n","  try:\n","    requests.get('https://httpbin.org/get')\n","    from tqdm import tqdm  # Import tqdm within the function to avoid potential errors\n","  except ModuleNotFoundError as e:\n","    print(f\"The following libraries are not installed: {', '.join(e.args)}\")\n","    print(\"Please install them using 'pip install requests tqdm'.\")\n","    return\n","\n","  # Prompt user for URL and filename (avoid providing a specific URL)\n","  url = \"https://pipeline-2.hotstorage.life/cfe85b1b9416b5ab38b0b684ed79a3284d61487474c6443c58e1030e65f0548f06635e7334b7745870f49d86f912c95609b4ba84ecf7b1ef6e131cb15a118e3a9d501059905e2e71cec7ae36bea5e03dbcf796715a74e63b571ae2d4f4bdada107b3eeaa036ebda032750f53d1ffd2d9c87adae06d7566a4c55c6baa8e21b2a392b4f21b731ceba0550046cee3f45c094216174340355b4908c980575b0d950a77bf04876efebe92ec151a67942e1ccb::95d3669530b51f2fbc5885221cbdb25a/Hanuman.2024.2160p.Jio.WEB-DL.DD5.1.H.264-Spidey.mkv\"\n","  filename = \"hanuman\"\n","\n","  try:\n","    response = requests.get(url, stream=True)\n","    response.raise_for_status()  # Raise an exception for unsuccessful requests\n","\n","    # Get file size (if available) for progress bar accuracy\n","    file_size = int(response.headers.get('content-length', None))\n","\n","    # Create a progress bar with meaningful description\n","    with tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024, desc=f\"Downloading: {filename}\") as progress_bar:\n","      with open(filename, 'wb') as f:\n","        for chunk in response.iter_content(1024):\n","          if chunk:  # filter out keep-alive new chunks\n","            f.write(chunk)\n","            progress_bar.update(len(chunk))  # Update progress bar based on chunk size\n","\n","    print(f\"MKV file downloaded successfully as '{filename}'.\")\n","  except requests.exceptions.RequestException as e:\n","    print(f\"An error occurred while downloading the file: {e}\")\n","\n","\n","if __name__ == \"__main__\":\n","  download_file(url=\"\", filename=\"\")\n"],"metadata":{"id":"qznC4p0DpGPG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get update && apt-get install -y ffmpeg\n"],"metadata":{"id":"0Sfe_1WIFbsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ffprobe -v error -select_streams v:0 -show_entries stream=codec_name,bit_rate /content/hanuman.mkv\n"],"metadata":{"id":"tyb9HNn5GG3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ffmpeg -i /content/hanuman.mkv -c:v libx264 -c:a copy -vf scale=1920:1080 output_1080p.mkv\n"],"metadata":{"id":"Z3o5HX2fGSN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ffprobe -v error -select_streams a: -show_entries stream=codec_name,channels,sample_rate /content/hanuman.mkv\n"],"metadata":{"id":"I3i1hZURHDIV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rv81s25VIdl4"},"execution_count":null,"outputs":[]}]}