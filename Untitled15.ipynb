{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPlLBGcjiAjqBRL81RMz3rY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"44wSz8n5gUKC"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","\n","# Generate synthetic data\n","X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=42)\n","\n","# Fit logistic regression models with L1 and L2 regularization\n","l1_model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0).fit(X, y)\n","l2_model = LogisticRegression(penalty='l2', solver='liblinear', C=1.0).fit(X, y)\n","\n","# Coefficients of the models\n","coef_l1 = l1_model.coef_.ravel()\n","intercept_l1 = l1_model.intercept_\n","\n","coef_l2 = l2_model.coef_.ravel()\n","intercept_l2 = l2_model.intercept_\n","\n","# Visualize the data and decision boundaries\n","plt.figure(figsize=(10, 6))\n","\n","# Plot data points\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', label='Data')\n","\n","# Plot decision boundaries\n","x_vals = np.linspace(X.min(), X.max(), 100)\n","y_vals_l1 = (-intercept_l1 - coef_l1[0] * x_vals) / coef_l1[1]\n","y_vals_l2 = (-intercept_l2 - coef_l2[0] * x_vals) / coef_l2[1]\n","\n","plt.plot(x_vals, y_vals_l1, 'r--', label='L1 Regularization')\n","plt.plot(x_vals, y_vals_l2, 'g-', label='L2 Regularization')\n","\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Logistic Regression with L1 and L2 Regularization')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the gradient descent function without regularization\n","def gradient_descent(X, y, alpha, num_iters):\n","    m, n = X.shape\n","    theta = np.zeros(n)\n","    costs = []\n","    for _ in range(num_iters):\n","        h = 1 / (1 + np.exp(-X.dot(theta)))\n","        gradient = X.T.dot(h - y) / m\n","        theta -= alpha * gradient\n","        cost = -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n","        costs.append(cost)\n","    return theta, costs\n","\n","# Generate synthetic data\n","np.random.seed(0)\n","X = np.random.rand(100, 2)\n","y = np.random.randint(2, size=100)\n","\n","# Add intercept term\n","X = np.hstack((np.ones((X.shape[0], 1)), X))\n","\n","# Gradient descent without regularization\n","theta_no_reg, costs_no_reg = gradient_descent(X, y, alpha=0.01, num_iters=1000)\n","\n","# Visualize coefficients without regularization\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(len(costs_no_reg)), costs_no_reg)\n","plt.title('Cost Function without Regularization')\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.show()\n","\n","# Define the gradient descent function with L1 regularization\n","def gradient_descent_l1(X, y, alpha, num_iters, lambd):\n","    m, n = X.shape\n","    theta = np.zeros(n)\n","    costs = []\n","    for _ in range(num_iters):\n","        h = 1 / (1 + np.exp(-X.dot(theta)))\n","        gradient = X.T.dot(h - y) / m\n","        theta -= alpha * (gradient + lambd * np.sign(theta))\n","        cost = -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / m) * np.sum(np.abs(theta))\n","        costs.append(cost)\n","    return theta, costs\n","\n","# Gradient descent with L1 regularization\n","theta_l1, costs_l1 = gradient_descent_l1(X, y, alpha=0.01, num_iters=1000, lambd=0.1)\n","\n","# Visualize coefficients with L1 regularization\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(len(costs_l1)), costs_l1)\n","plt.title('Cost Function with L1 Regularization')\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.show()\n","\n","# Define the gradient descent function with L2 regularization\n","def gradient_descent_l2(X, y, alpha, num_iters, lambd):\n","    m, n = X.shape\n","    theta = np.zeros(n)\n","    costs = []\n","    for _ in range(num_iters):\n","        h = 1 / (1 + np.exp(-X.dot(theta)))\n","        gradient = X.T.dot(h - y) / m\n","        theta -= alpha * (gradient + 2 * lambd * theta)\n","        cost = -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / m) * np.sum(theta**2)\n","        costs.append(cost)\n","    return theta, costs\n","\n","# Gradient descent with L2 regularization\n","theta_l2, costs_l2 = gradient_descent_l2(X, y, alpha=0.01, num_iters=1000, lambd=0.1)\n","\n","# Visualize coefficients with L2 regularization\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(len(costs_l2)), costs_l2)\n","plt.title('Cost Function with L2 Regularization')\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.show()\n"],"metadata":{"id":"6YbubNaOgXOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TAuNq3gZhvnC"},"execution_count":null,"outputs":[]}]}